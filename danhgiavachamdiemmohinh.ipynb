{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13859229,"sourceType":"datasetVersion","datasetId":8829048}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# 1. CÀI ĐẶT MÔI TRƯỜNG VÀ IMPORT THƯ VIỆN\n# ==============================================================================\n\n# Cài đặt các thư viện cần thiết cho Large Language Model (LLM) và đánh giá\n# - transformers: Thư viện chính để làm việc với các model HuggingFace.\n# - peft: Parameter-Efficient Fine-Tuning (để load LoRA adapter nhẹ hơn).\n# - bitsandbytes: Hỗ trợ lượng tử hóa (quantization) model xuống 4-bit/8-bit để chạy trên GPU thường.\n# - accelerate: Tối ưu hóa việc chạy model trên phần cứng (GPU/CPU).\n# - evaluate, rouge_score, bert_score: Các thư viện dùng để chấm điểm tóm tắt.\n!pip install -q -U transformers peft bitsandbytes accelerate\n!pip install -q -U evaluate rouge_score bert_score\n!pip install -q matplotlib seaborn\n\nimport torch\nimport json\nimport pandas as pd\nimport numpy as np\nimport evaluate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nfrom huggingface_hub import login\n\n# Cấu hình hiển thị pandas để xem được toàn bộ nội dung văn bản dài\npd.set_option('display.max_colwidth', None)\nprint(\"--> Đã cài đặt xong thư viện!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:42:37.819170Z","iopub.execute_input":"2025-11-26T06:42:37.819414Z","iopub.status.idle":"2025-11-26T06:44:40.279632Z","shell.execute_reply.started":"2025-11-26T06:42:37.819392Z","shell.execute_reply":"2025-11-26T06:44:40.278810Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-26 06:44:25.053993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764139465.240668      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764139465.293183      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"--> Đã cài đặt xong thư viện!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# 2. XÁC THỰC NGƯỜI DÙNG HUGGING FACE\n# ==============================================================================\n\n# --- CẤU HÌNH TOKEN ---\n# LƯU Ý BẢO MẬT: Không nên hardcode token trực tiếp nếu chia sẻ file công khai.\n# Hãy sử dụng Kaggle Secrets hoặc biến môi trường trong dự án thực tế.\nMY_TOKEN = \"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\" \n\n# Đăng nhập để có quyền truy cập vào các model (đặc biệt là Llama 3 cần xin quyền)\nlogin(token=MY_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:44:40.281453Z","iopub.execute_input":"2025-11-26T06:44:40.281950Z","iopub.status.idle":"2025-11-26T06:44:40.508118Z","shell.execute_reply.started":"2025-11-26T06:44:40.281929Z","shell.execute_reply":"2025-11-26T06:44:40.507030Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==============================================================================\n# 3. TẢI MODEL VÀ CẤU HÌNH LƯỢNG TỬ HÓA (QUANTIZATION)\n# ==============================================================================\n\n# Định nghĩa ID của model gốc và model đã tinh chỉnh (Adapter)\nbase_model_id = \"meta-llama/Meta-Llama-3.1-8B\"\nadapter_id = \"calmm-m/news-summarization\"\n\n# 1. Cấu hình 4-bit (BitsAndBytesConfig)\n# Mục đích: Giảm dung lượng VRAM cần thiết để chạy được model 8B trên GPU T4 của Kaggle.\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                   # Kích hoạt load 4-bit\n    bnb_4bit_use_double_quant=True,      # Lượng tử hóa kép để tiết kiệm thêm bộ nhớ\n    bnb_4bit_quant_type=\"nf4\",           # Dùng kiểu dữ liệu Normal Float 4 (tối ưu cho trọng số model)\n    bnb_4bit_compute_dtype=torch.float16 # Tính toán bằng float16 để tăng tốc độ\n)\n\n# 2. Tải Tokenizer\n# Tokenizer giúp chuyển văn bản thành các con số (tokens) mà model hiểu được.\nprint(\"Đang tải Tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.pad_token = tokenizer.eos_token # Thiết lập pad_token bằng eos_token để tránh lỗi padding\n\n# 3. Tải Base Model\nprint(\"Đang tải Base Model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=bnb_config, # Áp dụng cấu hình 4-bit đã định nghĩa\n    device_map=\"auto\"               # Tự động phân bổ layer model vào GPU/CPU\n)\n\n# 4. Tải Adapter (Fine-tuned)\n# PeftModel giúp ghép nối các trọng số LoRA (Adapter) vào model gốc mà không cần train lại toàn bộ.\nprint(\"Đang nạp Adapter (Fine-tuned)...\")\nft_model = PeftModel.from_pretrained(base_model, adapter_id)\n\nprint(\"--> HOÀN TẤT: Đã tải xong cả 2 mô hình!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:44:40.509017Z","iopub.execute_input":"2025-11-26T06:44:40.509313Z","iopub.status.idle":"2025-11-26T06:46:16.121578Z","shell.execute_reply.started":"2025-11-26T06:44:40.509283Z","shell.execute_reply":"2025-11-26T06:46:16.120975Z"}},"outputs":[{"name":"stdout","text":"Đang tải Tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2312d6526af4d84b964f2a5608625e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c35e8f1b68401998ed1cb9c5be4000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f977ba41d0754756ac05c7d77ce1661a"}},"metadata":{}},{"name":"stdout","text":"Đang tải Base Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb542feb1f647f5a4f5e7bc9c45201e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a8e04a3cef433196cf94984463af27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a5f2b3dd93e40ccb9ba0d46d50291be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0972d3e48f40469e8fb6cfe770ab3106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05c540ab960489592ba4e714c35e49a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df821e966614561b16c0f4436c8c11a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87eb8de97321404f979e1031eb849e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c245c177daf4e51ae9d558c9923c408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1d44625e714b118ce9583b9e7a482c"}},"metadata":{}},{"name":"stdout","text":"Đang nạp Adapter (Fine-tuned)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37427651fde4dccb0cc5d5a2db3103b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38bb218038945589c58c10fe720405f"}},"metadata":{}},{"name":"stdout","text":"--> HOÀN TẤT: Đã tải xong cả 2 mô hình!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==============================================================================\n# 4. HÀM SINH VĂN BẢN (INFERENCE)\n# ==============================================================================\n\ndef generate_smart(model, text):\n    \"\"\"\n    Thực hiện sinh văn bản tóm tắt dựa trên model và đầu vào được cung cấp.\n    Hàm này tự động phát hiện ngôn ngữ để chọn Prompt template phù hợp.\n\n    Args:\n        model (AutoModelForCausalLM): Mô hình ngôn ngữ (Base hoặc đã gắn Adapter).\n        text (str): Văn bản gốc cần tóm tắt.\n\n    Returns:\n        str: Văn bản tóm tắt do mô hình sinh ra (đã loại bỏ phần prompt).\n    \"\"\"\n    \n    # 1. Phát hiện ngôn ngữ để chọn Prompt phù hợp\n    # Kiểm tra sự xuất hiện của các ký tự đặc trưng tiếng Việt\n    vietnamese_chars = \"àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ\"\n    is_vietnamese = any(char in text.lower() for char in vietnamese_chars)\n    \n    # Cấu trúc Prompt (Zero-shot prompting)\n    if is_vietnamese:\n        prompt = f\"Hãy tóm tắt nội dung văn bản sau một cách ngắn gọn:\\n\\n{text}\\n\\nTóm tắt:\"\n        split_token = \"Tóm tắt:\"\n    else:\n        prompt = f\"Summarize the following text concisely:\\n\\n{text}\\n\\nSummary:\"\n        split_token = \"Summary:\"\n        \n    # 2. Mã hóa đầu vào và đưa lên GPU\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    # 3. Sinh văn bản (Generation)\n    with torch.no_grad(): # Tắt tính toán gradient để tiết kiệm bộ nhớ khi inference\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=150,      # Giới hạn độ dài tóm tắt tối đa\n            do_sample=True,          # Sử dụng sampling thay vì greedy search để văn bản tự nhiên hơn\n            temperature=0.4,         # Nhiệt độ thấp (0.4) giúp model tập trung vào thông tin chính xác, bớt sáng tạo/bịa đặt\n            top_p=0.9,               # Nucleus sampling: chỉ xét tập từ có xác suất tích lũy top 90%\n            repetition_penalty=1.2,  # Phạt nặng các từ lặp lại -> Giúp câu văn trôi chảy, tránh lặp vòng lặp\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # 4. Giải mã kết quả (Decode)\n    # Chuyển đổi tensor output trở lại thành chuỗi văn bản\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # 5. Hậu xử lý (Post-processing)\n    # Cắt bỏ phần Prompt ban đầu, chỉ lấy phần nội dung model sinh ra sau 'split_token'\n    if split_token in result:\n        return result.split(split_token)[-1].strip()\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:46:16.122442Z","iopub.execute_input":"2025-11-26T06:46:16.122802Z","iopub.status.idle":"2025-11-26T06:46:16.128719Z","shell.execute_reply.started":"2025-11-26T06:46:16.122775Z","shell.execute_reply":"2025-11-26T06:46:16.128028Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# 5. ĐỌC DỮ LIỆU VÀ CHẠY THỬ NGHIỆM\n# ==============================================================================\n\n# --- CẤU HÌNH INPUT ---\nINPUT_FILE = '/kaggle/input/datanew/train_data.jsonl' # Đường dẫn file dữ liệu\nNUM_SAMPLES = 100  # Số lượng mẫu thử nghiệm (giới hạn để tiết kiệm thời gian chạy)\n\ndef get_valid_text(value):\n    \"\"\"\n    Hàm phụ trợ để trích xuất văn bản an toàn từ dữ liệu JSON.\n    Xử lý trường hợp dữ liệu có thể là list hoặc string.\n    \"\"\"\n    if isinstance(value, list) and len(value) > 0:\n        return value[0]\n    if isinstance(value, str):\n        return value\n    return None\n\ndef load_data_flexible(file_path):\n    \"\"\"\n    Đọc dữ liệu từ file JSON hoặc JSONL một cách linh hoạt.\n    \n    Hàm này xử lý vấn đề định dạng file không nhất quán:\n    1. Thử đọc như một file JSON chuẩn (toàn bộ file là 1 list).\n    2. Nếu lỗi, thử đọc từng dòng (JSON Lines).\n\n    Args:\n        file_path (str): Đường dẫn tới file dữ liệu.\n\n    Returns:\n        list: Danh sách các object dữ liệu đã đọc được.\n    \"\"\"\n    data = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            # CÁCH 1: Thử đọc kiểu JSON chuẩn (Standard JSON)\n            try:\n                data = json.load(f)\n                print(\"-> Đã đọc file theo định dạng JSON chuẩn (List).\")\n            except json.JSONDecodeError:\n                # CÁCH 2: Nếu lỗi, chuyển sang đọc kiểu JSON Lines (từng dòng là 1 json object)\n                print(\"-> JSON chuẩn bị lỗi, đang chuyển sang đọc kiểu JSON Lines (từng dòng)...\")\n                f.seek(0) # Quan trọng: Quay lại đầu file để đọc lại từ đầu\n                for line in f:\n                    line = line.strip()\n                    if line:\n                        try:\n                            obj = json.loads(line)\n                            data.append(obj)\n                        except:\n                            continue # Bỏ qua dòng bị lỗi format\n    except FileNotFoundError:\n        print(f\"Lỗi: Không tìm thấy file tại {file_path}\")\n        return []\n    \n    return data\n\n# 1. Bắt đầu đọc file\ndata = load_data_flexible(INPUT_FILE)\n\n# Cắt dữ liệu lấy n mẫu đầu tiên\nif NUM_SAMPLES and len(data) > 0:\n    data = data[:NUM_SAMPLES]\n\nresults = []\nprint(f\"--> Bắt đầu chạy trên {len(data)} mẫu dữ liệu...\")\n\nif len(data) == 0:\n    print(\"CẢNH BÁO: Không đọc được dữ liệu nào! Hãy kiểm tra lại file input.\")\nelse:\n    # 2. Vòng lặp chạy tóm tắt (Sử dụng tqdm để hiển thị thanh tiến trình)\n    for item in tqdm(data):\n        # Trích xuất dữ liệu: \n        # - 'completion': văn bản gốc cần tóm tắt\n        # - 'prompt': bản tóm tắt mẫu (ground truth)\n        original_text = get_valid_text(item.get('completion'))\n        reference_sum = get_valid_text(item.get('prompt'))\n        \n        # Bỏ qua nếu dữ liệu bị thiếu hoặc rỗng\n        if not original_text or not reference_sum:\n            continue\n            \n        # Chạy inference trên cả 2 mô hình để so sánh\n        pred_base = generate_smart(base_model, original_text)\n        pred_ft = generate_smart(ft_model, original_text)\n        \n        results.append({\n            \"Input Text\": original_text,\n            \"Reference\": reference_sum,\n            \"Base Model\": pred_base,\n            \"Fine-tuned Model\": pred_ft\n        })\n\n    # 3. Tạo DataFrame và lưu kết quả tạm ra file CSV\n    df = pd.DataFrame(results)\n    df.to_csv(\"ket_qua_tho.csv\", index=False, encoding='utf-8-sig')\n    print(\"Đã chạy xong và lưu file 'ket_qua_tho.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T06:46:16.129490Z","iopub.execute_input":"2025-11-26T06:46:16.129680Z","iopub.status.idle":"2025-11-26T07:41:39.696118Z","shell.execute_reply.started":"2025-11-26T06:46:16.129665Z","shell.execute_reply":"2025-11-26T07:41:39.695348Z"}},"outputs":[{"name":"stdout","text":"-> JSON chuẩn bị lỗi, đang chuyển sang đọc kiểu JSON Lines (từng dòng)...\n--> Bắt đầu chạy trên 100 mẫu dữ liệu...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [55:17<00:00, 33.18s/it]","output_type":"stream"},{"name":"stdout","text":"Đã chạy xong và lưu file 'ket_qua_tho.csv'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# 6. ĐÁNH GIÁ VÀ TÍNH ĐIỂM (METRICS CALCULATION)\n# ==============================================================================\n\n# 1. Lọc sạch dữ liệu\n# Loại bỏ các dòng mà model sinh ra chuỗi rỗng hoặc dữ liệu tham chiếu bị rỗng\ndf_clean = df[\n    (df[\"Fine-tuned Model\"].str.strip() != \"\") & \n    (df[\"Reference\"].str.strip() != \"\")\n].copy()\n\nprint(f\"Số mẫu hợp lệ để chấm điểm: {len(df_clean)}\")\n\nif len(df_clean) > 0:\n    # 2. Load metrics\n    # - ROUGE: Đánh giá dựa trên sự trùng lặp n-gram (từ ngữ).\n    # - BERTScore: Đánh giá dựa trên ngữ nghĩa (embedding) dùng model BERT.\n    rouge = evaluate.load('rouge')\n    bertscore = evaluate.load('bertscore')\n\n    # Chuyển cột DataFrame thành List để đưa vào hàm compute\n    preds_base = df_clean[\"Base Model\"].tolist()\n    preds_ft = df_clean[\"Fine-tuned Model\"].tolist()\n    refs = df_clean[\"Reference\"].tolist()\n\n    # 3. Tính ROUGE Score\n    print(\"Đang tính ROUGE...\")\n    rouge_base = rouge.compute(predictions=preds_base, references=refs)\n    rouge_ft = rouge.compute(predictions=preds_ft, references=refs)\n\n    # 4. Tính BERTScore\n    # Lưu ý: lang=\"vi\" để sử dụng mô hình BERT hỗ trợ tiếng Việt\n    print(\"Đang tính BERTScore (Quá trình này có thể mất vài phút)...\")\n    bert_base = bertscore.compute(predictions=preds_base, references=refs, lang=\"vi\")\n    bert_ft = bertscore.compute(predictions=preds_ft, references=refs, lang=\"vi\")\n\n    # 5. In bảng so sánh kết quả\n    print(\"\\n\" + \"=\"*60)\n    print(f\"{'METRIC':<15} | {'BASE MODEL':<12} | {'FINE-TUNED':<12} | {'CẢI THIỆN'}\")\n    print(\"=\"*60)\n    \n    def print_metric(name, val_base, val_ft):\n        \"\"\"Hàm format và in một dòng kết quả so sánh.\"\"\"\n        diff = (val_ft - val_base) * 100 # Tính phần trăm cải thiện\n        print(f\"{name:<15} | {val_base:.4f}       | {val_ft:.4f}       | {diff:+.1f}%\")\n\n    # In các chỉ số ROUGE\n    print_metric(\"ROUGE-1\", rouge_base['rouge1'], rouge_ft['rouge1'])\n    print_metric(\"ROUGE-2\", rouge_base['rouge2'], rouge_ft['rouge2'])\n    print_metric(\"ROUGE-L\", rouge_base['rougeL'], rouge_ft['rougeL'])\n    print(\"-\" * 60)\n    \n    # In chỉ số BERTScore (Lấy trung bình cộng của F1 score trên toàn tập test)\n    print_metric(\"BERTScore (F1)\", np.mean(bert_base['f1']), np.mean(bert_ft['f1']))\n    print(\"=\"*60)\nelse:\n    print(\"Không có dữ liệu hợp lệ để tính điểm.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:39.697097Z","iopub.execute_input":"2025-11-26T07:41:39.697397Z","iopub.status.idle":"2025-11-26T07:41:54.689043Z","shell.execute_reply.started":"2025-11-26T07:41:39.697354Z","shell.execute_reply":"2025-11-26T07:41:54.688391Z"}},"outputs":[{"name":"stdout","text":"Số mẫu hợp lệ để chấm điểm: 93\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fcb1ddb216d437b98212479338fbfa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7615cac23b74c39821e014029dfcdb4"}},"metadata":{}},{"name":"stdout","text":"Đang tính ROUGE...\nĐang tính BERTScore...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd32026574c465ebf5b44e854bf995a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c673b180674f423ba588d3829a20ad60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a2dc7aba3c24b4caffeaf3fe44c3419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"191a9ab21d9f499f8e9af280d4e94a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d602962ac064ad68cb19c7bb495d343"}},"metadata":{}},{"name":"stderr","text":"Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nMETRIC          | BASE MODEL   | FINE-TUNED   | CẢI THIỆN\n============================================================\nROUGE-1         | 0.5095       | 0.5330       | +2.4%\nROUGE-2         | 0.3055       | 0.3140       | +0.9%\nROUGE-L         | 0.3554       | 0.3657       | +1.0%\n------------------------------------------------------------\nBERTScore (F1)  | 0.7124       | 0.7376       | +2.5%\n============================================================\n","output_type":"stream"}],"execution_count":6}]}