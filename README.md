# Qu√° tr√¨nh Fine-tuning cho d·ª± √°n n√†y

# 1. Dataset
   Ch√∫ng ta s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª´ Hugging Face: ƒë√¢y l√† d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch (https://huggingface.co/datasets/calmm-m/summarization)

# 2. Data Preparation Process
   Quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu g·ªìm 4 b∆∞·ªõc:

### Data Cleaning

   Lo·∫°i b·ªè c√°c th√¥ng tin kh√¥ng li√™n quan nh∆∞ m√£ s·∫£n ph·∫©m, k√Ω t·ª± ƒë·∫∑c bi·ªát v√† c√°c tr∆∞·ªùng th√¥ng tin kh√¥ng gi√∫p √≠ch cho vi·ªác d·ª± ƒëo√°n gi√°.

### Tokenization

   S·ª≠ d·ª•ng tokenizer c·ªßa m√¥ h√¨nh Llama


### Prompt Standardization

   Chu·∫©n h√≥a t·∫•t c·∫£ c√°c prompt theo c·∫•u tr√∫c:
   
    # `prompt` (y√™u c·∫ßu) l√† vƒÉn b·∫£n g·ªëc
    # `completion` (ƒë√°p √°n) l√† t√≥m t·∫Øt

# 4. Fine-tuning Techniques

   LoRA (Low-Rank Adaptation):
   T√†i li·ªáu: [LoRA Guide - Huggingface](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)
   
   QLoRA (Quantized LoRA):
   T√†i li·ªáu: [QLoRA Guide - Huggingface](https://huggingface.co/docs/peft/main/en/developer_guides/quantization)
   
   Paper tham kh·∫£o:
   üìÑ [QLoRA Paper (2023)](https://arxiv.org/pdf/2305.14314)

# 5. Important Hyperparameters

### 5.1 R (Rank)

   √ù nghƒ©a: X√°c ƒë·ªãnh k√≠ch th∆∞·ªõc kh√¥ng gian con trong LoRA (s·ªë l∆∞·ª£ng tham s·ªë th·∫•p h∆°n ƒë∆∞·ª£c hu·∫•n luy·ªán).
   
   Gi√° tr·ªã c√†ng nh·ªè ‚Üí √≠t tham s·ªë c·∫ßn c·∫≠p nh·∫≠t ‚Üí nhanh h∆°n, nh∆∞ng c√≥ th·ªÉ gi·∫£m hi·ªáu qu·∫£ fine-tuning.
   
   Docs: [PEFT LoRA Parameters](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)

### 5.2 Alpha

   √ù nghƒ©a: H·ªá s·ªë scale khu·∫øch ƒë·∫°i ·∫£nh h∆∞·ªüng c·ªßa c√°c tham s·ªë LoRA l√™n tr·ªçng s·ªë ban ƒë·∫ßu.
   
   Alpha c√†ng l·ªõn ‚Üí ·∫£nh h∆∞·ªüng LoRA c√†ng m·∫°nh m·∫Ω.
   
   Docs: [LoRA Alpha Scaling (g·ªëc LoRA paper).](https://arxiv.org/pdf/2106.09685.pdf)

### 5.3 Target Modules

   √ù nghƒ©a: C√°c module trong m√¥ h√¨nh m√† LoRA s·∫Ω √°p d·ª•ng.
   
   V√≠ d·ª•: "q_proj", "k_proj", "v_proj", "o_proj" (c√°c l·ªõp attention projection).
   
   Docs: [Choosing Target Modules](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#how-lora-works)

# 6. LLMs Training Process

### 6.1 Forward Pass

   ƒê∆∞a d·ªØ li·ªáu ƒë·∫ßu v√†o qua m√¥ h√¨nh ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n token ti·∫øp theo.

### 6.2 Loss Calculation

   So s√°nh ƒë·∫ßu ra c·ªßa m√¥ h√¨nh v·ªõi nh√£n th·ª±c t·∫ø ƒë·ªÉ t√≠nh to√°n loss.

### 6.3 Backward Pass

   T√≠nh to√°n gradient c·ªßa loss v·ªõi respect t·ªõi c√°c tham s·ªë m√¥ h√¨nh.

### 6.4 Optimization

   C·∫≠p nh·∫≠t tr·ªçng s·ªë theo c√¥ng th·ª©c:

**new_weight = old_weight - learning_rate * gradient**

üìö Docs: [Deep Learning Book - Optimization](https://www.deeplearningbook.org/contents/numerical.html)

# 7. Training Configuration

### 7.1 SFTTrainer Configuration

   Epochs: S·ªë l·∫ßn duy·ªát to√†n b·ªô t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán (c√≥ th·ªÉ tƒÉng l√™n 4+).
   
   Batch size: 1 (per device) √ó 8 (gradient accumulation) = 8 m·∫´u/l·∫ßn update.

### 7.2 Optimizer

   paged_adamw_32bit:
   
   Phi√™n b·∫£n AdamW t·ªëi ∆∞u b·ªô nh·ªõ khi fine-tune m√¥ h√¨nh l·ªõn.
   
   T√†i li·ªáu: [PagedAdamW Optimizer](https://huggingface.co/docs/bitsandbytes/main/en/reference/optim/adamw)

### 7.3 Learning Rate

   Tham s·ªë quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô c·∫≠p nh·∫≠t tr·ªçng s·ªë.
   
   N·∫øu qu√° cao ‚Üí c√≥ th·ªÉ nh·∫£y kh·ªèi ƒëi·ªÉm t·ªëi ∆∞u.
   
   N·∫øu qu√° th·∫•p ‚Üí m√¥ h√¨nh h·ªçc r·∫•t ch·∫≠m ho·∫∑c k·∫πt local minimum.

### 7.4 Scheduler

   Cosine scheduler:
   
   Learning rate gi·∫£m theo h√¨nh d·∫°ng s√≥ng cosine.
   
   B·∫Øt ƒë·∫ßu gi·∫£m nh·∫π ‚Üí gi·∫£m m·∫°nh ‚Üí ·ªïn ƒë·ªãnh d·∫ßn.
   
   Docs: [Cosine LR Scheduler](https://discuss.huggingface.co/t/using-cosine-lr-scheduler-via-trainingarguments-in-trainer/14783/6)



https://nvdam.widen.net/s/wlbgbqr7cj/nvidia-learning-training-course-catalog

Tai lieu tham khao trong video:
[Llm course](https://www.udemy.com/course/llm-engineering-master-ai-and-large-language-models/?srsltid=AfmBOorXXQrsRbJli2ye4GP3ICQCN5g5wUtofSmIx3ef6H6XqdbrC59P&couponCode=KEEPLEARNING)

**L∆∞u √Ω: To√†n b·ªô nh·ªØng g√¨ m√¨nh chia s·∫Ω ƒë·ªÅu l√† nh·ªØng g√¨ m√¨nh h·ªçc v√† t·ªïng h·ª£p ƒë∆∞·ª£c. No COMMERCIAL intent!!**
